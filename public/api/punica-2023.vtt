WEBVTT

1
00:00:00.800 --> 00:00:03.621
<v Eric>Welcome to Strollcast! I'm Eric.

2
00:00:03.921 --> 00:00:08.545
<v Maya>And I'm Maya. We're your AI hosts, here to make research accessible while you're on the move.

3
00:00:08.845 --> 00:00:20.574
<v Eric>Today we're diving into Punica, a system for serving multiple LoRA models efficiently. It's from Lequn Chen, Zihao Ye, and colleagues at the University of Washington and Duke.

4
00:00:20.874 --> 00:00:30.147
<v Maya>This paper tackles a problem that's becoming increasingly common: you have one base LLM, but many different fine-tuned versions for different users or tasks.

5
00:00:30.447 --> 00:00:40.505
<v Eric>Think of a platform where each customer has their own specialized LoRA adapter. Maybe one for legal documents, one for medical notes, one for code review.

6
00:00:40.805 --> 00:00:47.675
<v Maya>The naive approach is to load each LoRA model separately. But that's wasteful because they all share the same base model weights.

7
00:00:47.975 --> 00:00:59.756
<v Eric>Punica's insight is that you can batch requests across different LoRA models and process them together. The result is 12x higher throughput compared to existing serving systems.

8
00:01:00.056 --> 00:01:02.224
<v Maya>Let's break down how they achieved this.

9
00:01:03.324 --> 00:01:12.885
<v Eric>First, let's understand the problem. LoRA, or Low-Rank Adaptation, adds small trainable matrices to a frozen base model.

10
00:01:13.185 --> 00:01:18.383
<v Maya>Each LoRA adapter might only be a few megabytes. But the base model could be tens of gigabytes.

11
00:01:18.683 --> 00:01:29.263
<v Eric>If you serve LoRA models the traditional way, you load the base model plus one adapter, process requests, then swap to another adapter for different requests.

12
00:01:29.563 --> 00:01:37.321
<v Maya>That's inefficient for two reasons. First, you're not batching across users with different adapters. Second, swapping adapters has overhead.

13
00:01:37.621 --> 00:01:48.723
<v Eric>The ideal would be: load the base model once, keep multiple adapters in memory, and process requests for all of them simultaneously.

14
00:01:49.023 --> 00:01:55.789
<v Maya>But here's the challenge. During inference, LoRA adds a computation: the input gets multiplied by adapter matrices A and B.

15
00:01:56.089 --> 00:02:05.937
<v Eric>If different requests in your batch use different adapters, you can't just do one big matrix multiplication. Each request needs its own adapter weights.

16
00:02:06.237 --> 00:02:10.025
<v Maya>This is where Punica's key innovation comes in: the SGMV kernel.

17
00:02:11.125 --> 00:02:21.835
<v Eric>SGMV stands for Segmented Gather Matrix-Vector multiplication. It's a custom CUDA kernel designed specifically for this multi-LoRA scenario.

18
00:02:22.135 --> 00:02:29.789
<v Maya>Let's unpack what that means. Normally, when you batch requests in an LLM, you combine all the inputs and do one big matrix multiplication.

19
00:02:30.089 --> 00:02:43.229
<v Eric>With multiple LoRA adapters, you have a problem. Request 1 needs adapter A, request 2 needs adapter B, request 3 needs adapter A again, and so on.

20
00:02:43.529 --> 00:02:51.836
<v Maya>SGMV handles this by segmenting the batch. It groups requests by their LoRA adapter, then processes each segment with the appropriate weights.

21
00:02:52.136 --> 00:03:00.939
<v Eric>But here's the clever part: it does this within a single kernel launch. No separate kernel calls for each adapter.

22
00:03:01.239 --> 00:03:09.912
<v Maya>The kernel gathers the right adapter weights for each segment, performs the matrix-vector multiplications in parallel, then scatters the results back.

23
00:03:10.212 --> 00:03:20.661
<v Eric>This maintains high GPU utilization because you're still processing a large batch. You just route different parts of the batch through different adapters.

24
00:03:21.761 --> 00:03:24.947
<v Maya>Let's get a bit more technical on the SGMV implementation.

25
00:03:25.247 --> 00:03:41.208
<v Eric>The kernel has two variants: expand and shrink. In LoRA, you have two matrices per adapter. Matrix A expands the hidden dimension to the low-rank dimension, and matrix B shrinks it back.

26
00:03:41.508 --> 00:03:48.875
<v Maya>For the expand kernel, they parallelize across the output feature dimension. Each CUDA block handles a specific LoRA adapter.

27
00:03:49.175 --> 00:03:58.971
<v Eric>The shrink kernel is trickier because the output dimension is small, often just 8 or 16. Not enough parallelism there.

28
00:03:59.271 --> 00:04:05.854
<v Maya>So they use a split-K strategy. They divide the input dimension across multiple blocks and do a reduction at the end.

29
00:04:06.154 --> 00:04:14.173
<v Eric>Both kernels leverage GPU tensor cores for the actual matrix multiplication. That's critical for performance.

30
00:04:14.473 --> 00:04:19.933
<v Maya>The result is that batching different LoRA models has nearly the same throughput as batching identical models.

31
00:04:20.233 --> 00:04:33.791
<v Eric>In their experiments, they saw negligible performance difference between the "identical" case where all requests use the same adapter and the "distinct" case where every request uses a different adapter.

32
00:04:34.091 --> 00:04:38.348
<v Maya>That's remarkable. It means you get efficient multi-tenant serving essentially for free.

33
00:04:39.448 --> 00:04:47.625
<v Eric>SGMV is the core kernel, but Punica is a complete serving system. Let's talk about the architecture.

34
00:04:47.925 --> 00:04:53.071
<v Maya>There are three main components: the scheduler, the SGMV execution layer, and memory management.

35
00:04:53.371 --> 00:05:02.592
<v Eric>The scheduler receives incoming requests, each tagged with which LoRA adapter it needs. It groups requests into batches for processing.

36
00:05:02.892 --> 00:05:09.658
<v Maya>The key scheduling decision is how to compose batches. You want to maximize GPU utilization while keeping latency reasonable.

37
00:05:09.958 --> 00:05:18.213
<v Eric>Punica batches requests across different LoRA models within a single iteration. This is what enables the high throughput.

38
00:05:18.513 --> 00:05:24.130
<v Maya>For memory management, Punica caches adapter weights on the GPU. Popular adapters stay resident.

39
00:05:24.430 --> 00:05:32.136
<v Eric>Less popular adapters can be loaded on demand. The loading latency is only milliseconds because adapters are small.

40
00:05:32.436 --> 00:05:38.000
<v Maya>This gives Punica flexibility. You're not constrained by which adapters are currently loaded on a GPU.

41
00:05:38.300 --> 00:05:48.514
<v Eric>The system can consolidate requests to a small set of GPUs, loading adapters as needed, rather than pinning specific adapters to specific GPUs.

42
00:05:49.614 --> 00:05:54.159
<v Maya>Let's talk numbers. How much does Punica actually improve over existing systems?

43
00:05:54.459 --> 00:06:00.754
<v Eric>They compared against vLLM, which was the state-of-the-art LLM serving system at the time.

44
00:06:01.054 --> 00:06:08.238
<v Maya>In the multi-LoRA scenario, Punica achieved 12x higher throughput while adding only 2 milliseconds of latency per token.

45
00:06:08.538 --> 00:06:19.118
<v Eric>That 2 milliseconds is the overhead from the SGMV kernel compared to standard matrix multiplication. It's negligible for most applications.

46
00:06:19.418 --> 00:06:28.430
<v Maya>They tested various workload distributions. "Distinct" where every request uses a different adapter. "Identical" where all requests use the same one.

47
00:06:28.730 --> 00:06:39.858
<v Eric>Also "Uniform" where adapters are equally popular, and "Skewed" where popularity follows a Zipf distribution, meaning a few adapters get most of the traffic.

48
00:06:40.158 --> 00:06:48.230
<v Maya>Across all distributions, Punica maintained high throughput. The skewed case was actually easiest because popular adapters stay hot in cache.

49
00:06:48.530 --> 00:06:56.132
<v Eric>They also tested with different numbers of concurrent LoRA models. Punica scaled well up to hundreds of different adapters.

50
00:06:56.432 --> 00:07:03.250
<v Maya>Memory efficiency was another win. By sharing the base model and only storing small adapters, GPU memory goes much further.

51
00:07:04.350 --> 00:07:12.134
<v Eric>Let's zoom out and discuss the implications. Why is multi-tenant LoRA serving important?

52
00:07:12.434 --> 00:07:19.722
<v Maya>The rise of LoRA has changed how organizations customize LLMs. Instead of fine-tuning the whole model, you train small adapters.

53
00:07:20.022 --> 00:07:28.643
<v Eric>A single base model can have hundreds or thousands of LoRA adapters for different tasks, domains, or customers.

54
00:07:28.943 --> 00:07:35.761
<v Maya>Think of it like a platform. The platform provider hosts the base LLM. Each customer uploads their specialized adapter.

55
00:07:36.061 --> 00:07:47.894
<v Eric>Without Punica, serving this efficiently was hard. You'd either dedicate GPUs per customer, which is wasteful, or swap adapters constantly, which is slow.

56
00:07:48.194 --> 00:07:55.117
<v Maya>Punica enables true multi-tenancy. One GPU cluster serves all customers efficiently, dynamically loading adapters as needed.

57
00:07:55.417 --> 00:08:01.947
<v Eric>This reduces the cost per customer dramatically. It makes personalized LLMs economically viable.

58
00:08:02.247 --> 00:08:09.588
<v Maya>And it's not just about cost. Lower latency means better user experience. Higher throughput means serving more users.

59
00:08:10.688 --> 00:08:18.211
<v Eric>Punica fits into a broader ecosystem of LLM serving optimizations. Let's connect it to related work.

60
00:08:18.511 --> 00:08:27.941
<v Maya>vLLM introduced PagedAttention for efficient KV cache management. Punica is complementary; it optimizes the LoRA computation, not the attention.

61
00:08:28.241 --> 00:08:38.586
<v Eric>You could combine Punica's SGMV with vLLM's memory management. In fact, vLLM has since incorporated similar multi-LoRA support.

62
00:08:38.886 --> 00:08:43.797
<v Maya>There's also work on LoRA weight compression. If adapters are smaller, you can cache more of them.

63
00:08:44.097 --> 00:08:54.258
<v Eric>And techniques like LoRA merging, where you combine multiple adapters into one, can reduce the number of distinct adapters at the cost of some specificity.

64
00:08:54.558 --> 00:09:01.141
<v Maya>Punica's approach is orthogonal to these. It doesn't require changing the adapters themselves, just how you batch the computation.

65
00:09:01.441 --> 00:09:11.498
<v Eric>The SGMV kernel is open source. Other serving systems have adopted similar techniques since the paper came out.

66
00:09:12.598 --> 00:09:16.778
<v Maya>For listeners thinking about deploying this, let's cover some practical points.

67
00:09:17.078 --> 00:09:25.437
<v Eric>First, Punica is most beneficial when you have many different LoRA adapters. If you only have one or two, the overhead isn't worth it.

68
00:09:25.737 --> 00:09:33.391
<v Maya>The crossover point depends on your workload, but roughly: if you're frequently switching between more than a handful of adapters, Punica helps.

69
00:09:33.691 --> 00:09:41.998
<v Eric>Adapter size matters for memory planning. Typical LoRA adapters for a 7B model might be 10 to 50 megabytes.

70
00:09:42.298 --> 00:09:49.011
<v Maya>For a 70B model, adapters could be a few hundred megabytes. Still small compared to the base model, but they add up.

71
00:09:49.311 --> 00:09:58.951
<v Eric>Loading latency is proportional to adapter size divided by PCIe bandwidth. With NVLink or on-GPU cache, it's faster.

72
00:09:59.251 --> 00:10:04.710
<v Maya>The scheduler's batching strategy can be tuned. Larger batches improve throughput but increase latency.

73
00:10:05.010 --> 00:10:12.298
<v Eric>In practice, you want to set latency SLOs and let the scheduler maximize throughput within those bounds.

74
00:10:13.398 --> 00:10:15.984
<v Maya>Every system has limitations. What are Punica's?

75
00:10:16.284 --> 00:10:26.133
<v Eric>First, SGMV has some overhead compared to fused operations for single-LoRA serving. If you only ever use one adapter, don't use Punica.

76
00:10:26.433 --> 00:10:35.863
<v Maya>Second, the current implementation focuses on LoRA specifically. Other adapter types like adapters from AdapterHub or prefix tuning would need different kernels.

77
00:10:36.163 --> 00:10:44.705
<v Eric>Third, very large batches with many distinct adapters can reduce arithmetic intensity. The segments become too small.

78
00:10:45.005 --> 00:10:51.170
<v Maya>In practice, they found this wasn't a problem for realistic workloads. But it's a theoretical limitation.

79
00:10:51.470 --> 00:11:00.926
<v Eric>Finally, Punica assumes adapters are available when requests arrive. If you need to train new adapters on demand, that's a different problem.

80
00:11:02.026 --> 00:11:04.429
<v Maya>Let's summarize what we've learned about Punica.

81
00:11:04.729 --> 00:11:12.279
<v Eric>The core insight is that multi-tenant LoRA serving can be efficient if you batch across adapters intelligently.

82
00:11:12.579 --> 00:11:19.214
<v Maya>The SGMV kernel enables this by segmenting batches and processing each segment with the appropriate adapter weights.

83
00:11:19.514 --> 00:11:29.545
<v Eric>The kernel uses GPU tensor cores, split-K parallelization, and careful memory access patterns to maintain high throughput.

84
00:11:29.845 --> 00:11:34.338
<v Maya>The full Punica system adds scheduling, memory management, and on-demand adapter loading.

85
00:11:34.638 --> 00:11:41.587
<v Eric>The result is 12x higher throughput than naive approaches, with only 2 milliseconds additional latency.

86
00:11:41.887 --> 00:11:48.757
<v Maya>This makes personalized LLMs economically viable. One GPU cluster can serve hundreds of specialized adapters.

87
00:11:49.057 --> 00:12:00.159
<v Eric>The code is open source at punica-ai/punica on GitHub. Similar techniques have been adopted by vLLM and other serving systems.

88
00:12:00.459 --> 00:12:06.206
<v Maya>If you're building a platform with many fine-tuned models, multi-tenant serving is something to think about.

89
00:12:06.506 --> 00:12:15.936
<v Eric>The paper is on arXiv at 2310.18547, and was published at MLSys 2024.

90
00:12:17.036 --> 00:12:19.962
<v Eric>Time to test your understanding with a couple of quizzes.

91
00:12:20.262 --> 00:12:27.680
<v Maya>Quiz: What is the main challenge that prevents naive batching when serving multiple different LoRA adapters, and how does SGMV solve it?

92
00:12:27.980 --> 00:12:29.887
<v Eric>Take a moment to think about it.

93
00:12:30.187 --> 00:12:58.609
<v Maya>Answer: The main challenge is that different requests in a batch need different adapter weights. You can't do one big matrix multiplication because each request's LoRA computation requires its own A and B matrices. SGMV solves this by segmenting the batch, grouping requests by their adapter, and processing all segments in a single kernel launch. It gathers the appropriate weights for each segment, performs parallel matrix-vector multiplications, and scatters results back.

94
00:12:59.709 --> 00:13:01.407
<v Eric>Here's quiz number two.

95
00:13:01.707 --> 00:13:06.957
<v Maya>Quiz: Why does Punica use a split-K strategy for the shrink kernel but not for the expand kernel?

96
00:13:07.257 --> 00:13:10.496
<v Eric>Think back to what we covered about the kernel implementation.

97
00:13:10.796 --> 00:13:39.949
<v Maya>Answer: The shrink kernel reduces from the hidden dimension to the low-rank dimension, which is typically small, like 8 or 16. There's not enough parallelism in such a small output dimension to keep the GPU busy. Split-K divides the work along the input dimension instead, giving each block a portion to process, then combines results with a reduction. The expand kernel doesn't need this because it outputs to the full hidden dimension, which provides plenty of parallelism naturally.

98
00:13:41.049 --> 00:13:43.583
<v Eric>That's all for today's episode on Punica.

99
00:13:43.883 --> 00:13:47.462
<v Maya>Thanks for joining us on this exploration of efficient multi-LoRA serving.

100
00:13:47.762 --> 00:13:50.348
<v Eric>Until next time, keep strolling.

101
00:13:50.648 --> 00:13:52.764
<v Maya>And may your gradients never explode.
