{
  "version": "1.0",
  "updated": "2025-12-26T00:00:00Z",
  "episodes": [
    {
      "id": "punica-2023",
      "title": "Punica: Multi-Tenant LoRA Serving",
      "authors": "Chen et al.",
      "year": 2023,
      "duration": "14 min",
      "description": "Efficient multi-tenant LoRA serving via SGMV kernels: batch requests across different adapters for 12x higher throughput.",
      "audioPath": "/chen-2023-punica/chen-2023-punica.m4a",
      "transcriptPath": "/api/punica-2023.vtt",
      "paperUrl": "https://arxiv.org/abs/2310.18547"
    },
    {
      "id": "qlora-2023",
      "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
      "authors": "Dettmers et al.",
      "year": 2023,
      "duration": "15 min",
      "description": "Democratizing LLM fine-tuning: 4-bit quantization plus LoRA enables training 65B models on a single GPU.",
      "audioPath": "/dettmers-2023-qlora/dettmers-2023-qlora.m4a",
      "transcriptPath": "/api/qlora-2023.vtt",
      "paperUrl": "https://arxiv.org/abs/2305.14314"
    },
    {
      "id": "gated-attention-2025",
      "title": "Gated Attention for Large Language Models",
      "authors": "Qiu et al.",
      "year": 2025,
      "duration": "35 min",
      "description": "NeurIPS 2025 Best Paper: A simple sigmoid gate after attention eliminates attention sinks, improves training stability, and enables million-token contexts.",
      "audioPath": "/qiu-2025-gated-attention/qiu-2025-gated-attention.m4a",
      "paperUrl": "https://arxiv.org/abs/2505.06708"
    },
    {
      "id": "pathways-2022",
      "title": "Pathways: Asynchronous Distributed Dataflow for ML",
      "authors": "Barham et al.",
      "year": 2022,
      "duration": "29 min",
      "description": "Google's orchestration layer for accelerators using asynchronous dataflow, enabling flexible parallelism across thousands of TPUs.",
      "audioPath": "/barham-2022-pathways/barham-2022-pathways.m4a",
      "paperUrl": "https://arxiv.org/abs/2203.12533"
    },
    {
      "id": "megatron-lm-2021",
      "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM",
      "authors": "Narayanan et al.",
      "year": 2021,
      "duration": "34 min",
      "description": "NVIDIA's techniques for training trillion-parameter models across thousands of GPUs using tensor, pipeline, and data parallelism.",
      "audioPath": "/narayanan-2021-megatron-lm/narayanan-2021-megatron-lm.m4a",
      "paperUrl": "https://arxiv.org/abs/2104.04473"
    },
    {
      "id": "pytorch-fsdp-2023",
      "title": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel",
      "authors": "Zhao et al.",
      "year": 2023,
      "duration": "24 min",
      "description": "Meta's production experiences building fully sharded data parallel training into PyTorch.",
      "audioPath": "/zhao-2023-pytorch-fsdp/zhao-2023-pytorch-fsdp.m4a",
      "paperUrl": "https://arxiv.org/abs/2304.11277"
    },
    {
      "id": "zero-2020",
      "title": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models",
      "authors": "Rajbhandari et al.",
      "year": 2020,
      "duration": "17 min",
      "description": "Microsoft's breakthrough technique for eliminating memory redundancy in distributed training, enabling trillion-parameter models.",
      "audioPath": "/rajbhandari-2020-zero/rajbhandari-2020-zero.m4a",
      "paperUrl": "https://arxiv.org/abs/1910.02054"
    }
  ]
}
